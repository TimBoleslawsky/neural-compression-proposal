\section{Approach}\label{sec:approach}

% Justification of tokenization as a neural compression strategy - Done
\textbf{Justification:} As introduced in Section~\ref{sec:context}, neural compression techniques leverage deep learning to enhance data distribution modeling capabilities as well as enable learned transform coding and quantization \citep{yang2022}. It has also been discussed, how neural compression methods often rely on computationally heavy architectures like RNNs and transformers \citep{zheng2023, löhdefink2019, kawawabeaudan2022, liu2024deepdictdeeplearningbased}, to solve the task of entropy modeling within the compression pipeline. The prevalence of these complex architectures is largely due to two factors. First, traditionally compression pipelines utilize continuous latent representations, because these are much easier to optimize end-to-end with gradient descent. However, accurately modeling the entropy of continuous latents requires sophisticated context models capable of capturing long-range dependencies.~\citet{ballé2018} for example introduce a sophisticated but complex hyperprior to better capture these long-range dependencies and discuss the challenges. Second, discrete latent learning used to be unstable and difficult to implement \citep{vandenoord2017neural}. % TODO: Be more concrete on why?

The idea to use discrete latent representations within the transform step of the compression pipeline to simplify the entropy modeling task and therefore enable more efficient compression is the main inspiration behind the Vector Quantized-Variational AutoEncoder (VQ-VAE) architecture of \citet{vandenoord2017neural}, which proposes the use of vector quantization as a way to learn discrete latent spaces. The VQ-VAE architecture and its successors have been successfully applied to image and audio data, but their main focus remains reconstruction \citep{vandenoord2017neural, razavi2019generating}. This makes them suboptimal for task-aware compression tasks. Tokenization emerges as an alternative approach to produce discrete latent representations in audio and speech processing research \citep{schmidt2024tokenization}. % Wgy VQ-VAE?

Tokenization is traditionally understood as the mapping of high-dimensional, continuous inputs into a sequence of discrete symbols drawn from a finite vocabulary \citep{grefenstette1999}. Tokenization therefore can act as a form of transformation and quantization: it reduces dimensionality, decorrelates, and constrains representations to a compact code space. Additionally, tokenization can be made task-aware so that the retained tokens are maximally useful for prediction or classification. One example of this is the WavTokenizer by \citet{ji2025}, which efficiently tokenizes acoustic data for audio language modeling. We propose, that this idea can be translated to time series data to enable lightweight entropy modeling architectures. This would allow more computationally efficient compression pipelines, which, as shown, is especially relevant for in-vehicle embedded systems with limited computational resources.

% Proposed Approach - decide on a baseline and formulate
\textbf{Detailed Approach:} The proposed approach is to develop a task-aware tokenization framework for automotive time series data compression that balances computational efficiency, compression rate, and ML utility. 
\begin{itemize}
    \item \textbf{Dataset:} As a dataset the focus will be on available automotive sensor and telemetry test-fleet data supporting tasks such as predictive maintenance and anomaly detection. Alternatively, publicly available datasets such as the SCANIA Component X Dataset can be used \citep{kharazian2025}.
    \item \textbf{Task 1:} Train downstream ML models on uncompressed data to quantify loss in predictive utility.
    \item \textbf{Task 2:} Implement baseline. % What to use?
    \item \textbf{Task 3:} Develop a learnable tokenization module that discretizes data into semantically meaningful units optimized for downstream tasks.
    \item \textbf{Task 4:} Develop lightweight entropy modeling and coding schemes tailored to the tokenized representations.
    \item \textbf{Task 5:} Evaluate and compare the methods.
    \begin{itemize}
        \item Measure rate-utility curves across the methods.
        \item Evaluate trade-offs between computational efficiency.
    \end{itemize}
    \item \textbf{Expected Outcome:} Demonstrate that task-aware tokenization achieves comparable rate-utility trade-off to established neural compression approaches, while increasing computational efficiency. 
\end{itemize}