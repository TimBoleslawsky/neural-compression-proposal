\section{Background}\label{sec:background} 

In essence, event-triggered logging is a form of compression that reduces data volume by selectively recording only significant events. Given the limitations of this approach in automotive systems and the need for ML-ready data, one might look to traditional compression methods for an alternative.

\textbf{Traditional Compression:} Compression, as originated in information theory by \citet{shannon1948}, is the process of encoding information using fewer bits than the original representation. Compression techniques can be broadly categorized into lossless and lossy methods. Lossless compression is based on two principles: distribution modelling, sometimes called entropy modeling, and entropy coding. Entropy modeling involves creating a probabilistic representation of the data, while entropy coding assigns shorter codes to more frequent symbols based on their probabilities, thereby minimizing the average code length. Lossy compression allows for some loss of information in exchange for higher compression ratios. This is typically achieved through techniques such as transform coding and quantization \citep{sayood2018}. For the purpose of this project, the focus will be on lossy compression as we focus on downstream ML tasks where some loss of fidelity is acceptable as long as the relevant information for the task is preserved.

Traditional compression methods, based on these information theory principles, often fall short in automotive applications, especially as a precursor for downstream ML tasks. For video/image compression traditional methods like JPEG or MP3 are optimized for human perception (e.g., visual quality) rather than ML tasks or efficient downstream data use \citep{ma2019}. For time series data, algorithmic approaches like CHIMP or Gorilla depend on manually chosen parameters like window size and are sensitive to data characteristics such as entropy and signal variability. This limits their effectiveness in capturing the nuances required for accurate ML model performance in automotive contexts \citep{johnsson2025thesis}. These algorithmic approaches were investigated by \citet{johnsson2025thesis} in a previous master thesis project. This work builds upon this thesis by exploring an alternative approach to compressing vehicle telemetry data.

\textbf{Rate-Utility Trade-off and Related Research:} As introduced in Section~\ref{sec:context}, constructing downstream ML models for automotive systems, or in fact Internet-of-Things (IoT) systems in general, is a constant trade-off between handling large quantities of data and maximizing model performance. Traditional compression techniques can reduce data volume, but often at the cost of losing critical information necessary for accurate ML tasks such as predictive maintenance, anomaly detection, and fleet analytics. The impact of this trade-off is well-documented in the literature.~\citet{cuza2024} for example study the impact of lossy compression techniques on time series forecasting tasks and observe a constant trade-off between compression ratio and forecasting accuracy.

Existing research approaches these challenges from three different angles: utility-aware adaptive telemetry, neural compression, and task-aware compression. 

\begin{itemize}
    \item First, utility-aware adaptive telemetry methods aim to employ policy learning methods to dynamically adjust telemetry parameters to reduce maintenance costs while preserving data utility for downstream tasks. Although this approach is still emerging, recent research has demonstrated promising results \citep{zhang2023adapint}.
    \item  Second, neural compression techniques learn data representations optimized for both compression efficiency and ML task performance. This research is heavily inspired by deep generative models like GANs, VAEs, and autoregressive models, but focuses on compressing the data, instead of generating realistic data samples \citep{yang2022}. Neural compression techniques extend the introduced lossy compression principles in two key ways. First, they offer an alternative to traditional distribution modelling by leveraging deep neural networks to learn complex data distributions directly from the data, capturing intricate patterns and dependencies that traditional statistical models may miss. Second, they substitute traditional approaches to transform coding and quantization with learned representations \citep{yang2022}. Studies as early as 2019 have shown that neural compression methods can outperform traditional compression techniques for image and video data, especially at low bitrates \citep{löhdefink2019}. The same has been shown for time series data \citep{zheng2023, liu2024deepdictdeeplearningbased}. 
    \item Lastly, task-aware compression techniques focus on optimizing compression algorithms to retain information that is most relevant for specific tasks \citep{yang2022}. This idea has shown promise in handling time-series data more efficiently in IoT systems.~\citet{azar2022robust} and \citet{sun2025} for example explore task-aware compression algorithms that adaptively prioritize data features based on their relevance to downstream tasks, demonstrating improved performance in resource-constrained environments.
\end{itemize}

When combining task-aware methods and neural compression methods, task-aware neural compression models have shown promise in reducing the rate-utility trade-off. These models are specifically designed to retain essential features for ML tasks while achieving high compression ratios \citep{yang2022}. Studies that empirically evaluate the performance of task-aware neural compression models are somewhat limited, but they do exist. In one study for example, \citet{kawawabeaudan2022} use a hierarchical autoencoder-based compression network together with a recognition model and implement two hyperparameters to trade off between distortion, bitrate, and recognition performance. 

An important note that needs to be made on neural compression methods is that they often rely on computationally heavy architectures like RNNs and transformers \citep{zheng2023, löhdefink2019, kawawabeaudan2022, liu2024deepdictdeeplearningbased}, to solve the task of entropy modeling within the compression pipeline. The prevalence of these complex architectures is largely due to the complexity of the continuous latent space. Traditionally compression pipelines utilize continuous latent representations because these are much easier to optimize end-to-end with gradient descent. However, accurately modeling the entropy of continuous latents requires sophisticated context models capable of capturing long-range dependencies.~\citet{ballé2018} for example introduce a sophisticated but complex hyperprior to better capture these long-range dependencies and discuss the challenges.
