\section{Motivation}\label{sec:motivation}

The basic motivation behind using tokenization as a neural compression strategy for automotive time series data is to produce discrete latent representations that simplify the entropy modeling task within the compression pipeline. By constraining the data to a finite set of tokens, the complexity of modeling the underlying data distribution is reduced, enabling the use of lightweight entropy models that are computationally efficient.

The idea to use discrete latent representations within the transform step of the compression pipeline to simplify the entropy modeling task and therefore enable more efficient compression has already been explored in recent years. It is the main inspiration behind the Vector Quantized-Variational AutoEncoder (VQ-VAE) architecture of \citet{vandenoord2017neural}, one of the most prevalent methods to produce discrete latent representations.~\citet{vandenoord2017neural} propose the use of vector quantization as a way to learn discrete latent spaces. The VQ-VAE architecture and its successors have been successfully applied to image and audio data, but their main focus remains reconstruction \citep{vandenoord2017neural, razavi2019generating}. This makes them suboptimal for task-aware compression tasks. Tokenization emerges as an alternative approach to produce discrete latent representations in audio and speech processing research \citep{schmidt2024tokenization}.

Tokenization is traditionally understood as the mapping of high-dimensional, continuous inputs into a sequence of discrete symbols drawn from a finite vocabulary \citep{grefenstette1999}. Tokenization therefore can act as a form of transformation and quantization: it reduces dimensionality, decorrelates, and constrains representations to a compact code space. Additionally, tokenization can be made task-aware so that the retained tokens are maximally useful for prediction or classification. One example of this is the WavTokenizer by \citet{ji2025}, which efficiently tokenizes acoustic data for audio language modeling. We propose, that this idea can be translated to time series data to enable lightweight entropy modeling architectures. This would allow more computationally efficient compression pipelines, which, as shown, is especially relevant for in-vehicle embedded systems with limited computational resources.

