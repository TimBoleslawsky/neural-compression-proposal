\section{Motivation}\label{sec:motivation}

The motivation behind using tokenization as a neural compression strategy for automotive time-series data is to produce discrete latent representations that simplify the transform and quantize stage of the compression pipeline. By constraining the data to a finite set of tokens, the computational load of encoding can be reduced, enabling more efficient compression for in-vehicle systems.

Historically, heavier transform models such as RNNs and Transformers have been used within the neural compression context, especially with time-series data, because they can capture long-range dependencies in sequential data effectively and are easily optimized using gradient descent~\cite{hochreiter1997long, vaswani2017attention}. 

In recent years, the idea to use discrete latent representations as a means to represent high-dimensional data efficiently in generative modeling has been explored. It is the main inspiration behind the Vector Quantized-Variational AutoEncoder (VQ-VAE) architecture of \citet{vandenoord2017neural}, one of the most prevalent methods to produce discrete latent representations.~\citet{vandenoord2017neural} propose the use of vector quantization as a way to learn discrete latent spaces. The VQ-VAE architecture and its successors have been successfully applied to image and audio data, but their main focus remains reconstruction \citep{vandenoord2017neural, razavi2019generating}. This makes them suboptimal for task-aware compression tasks. Tokenization emerges as an alternative approach to produce discrete latent representations in audio and speech processing research \citep{schmidt2024tokenization}.

Tokenization is traditionally understood as the mapping of high-dimensional, continuous inputs into a sequence of discrete symbols drawn from a finite vocabulary \citep{grefenstette1999}. Tokenization therefore can act as a form of transformation and quantization: it reduces dimensionality, decorrelates, and constrains representations to a compact code space. Additionally, tokenization can be made task-aware so that the retained tokens are maximally useful for prediction or classification. One example of this is the WavTokenizer by \citet{ji2025}, which efficiently tokenizes acoustic data for audio language modeling. We propose that this idea can be translated to time-series data to reduce the computational cost of the transform/quantize stage, enabling more efficient encoding on in-vehicle embedded systems with limited computational resources.
