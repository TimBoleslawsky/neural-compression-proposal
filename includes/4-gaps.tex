\section{Problems \& Research Gaps}\label{sec:research-gaps}

Analyzing relevant industry practices and literature on compression techniques reveals several significant unsolved challenges and research gaps, which will be addressed with this project:

\begin{itemize}
    \item From the industry perspective, automotive systems need high-utility ML-ready data under severe bandwidth and computational limits. Existing event-triggered logging schemes introduce sampling bias and maintenance overhead.
    \item While there exists some exploration of task-aware neural compression techniques for image and video data, there is a notable lack of research focusing on time-series data, which is the predominant data type in automotive and IoT applications. This gap is supported by a 2022 survey done on the topic of neural compression \citep{yang2022}.
    \item Many of the reviewed papers focus primarily on maximizing compression ratios while preserving accuracy but do not explicitly evaluate the computational efficiency of the produced methods. In neural compression for time-series data, lightweight convolutional encoders (e.g., TCNs) have been used \citep{zheng2023}, but transformer-based encoders are predominantly employed in the transform step \citep{liu2024deepdictdeeplearningbased}. The computational complexity of transformer encoders, particularly due to attention operations, can make real-time deployment on embedded systems challenging, yet encode-time runtime, memory usage, and FLOPs are rarely reported.
\end{itemize}

While task-aware approaches to modern compression techniques like neural compression have shown promising advancements in balancing the rate-utility trade-off, there remains a significant gap in analyzing their effects on time series data, specifically in vehicular contexts, where computational resources and bandwidth are often constrained. A lightweight, task-aware compression method for automotive time-series data is therefore needed. Based on the existing literature there are three relevant research questions which this project aims to answer.

\begin{itemize}
    \item \textbf{RQ1:} How can the use of tokenization as an alternative transform and quantization strategy enable the use of more lightweight entropy models?
    \item \textbf{RQ2:} What effects does the use of tokenization as a transform and quantization strategy have on the rate-utility trade-off in the case of time-series data?
    \item \textbf{RQ3:} How well can a compression method, using tokenization, be fine-tuned for a specific task-type while generalizing well across different tasks of this task-type?
\end{itemize}




