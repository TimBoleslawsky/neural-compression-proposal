\section{Context}\label{sec:context} 

% The in-vehicle embedded systems - Done
\textbf{The In-Vehicle Embedded System:} An in-vehicle embedded system is a specialized computer system integrated within a vehicle to perform dedicated functions, often in real time, and is essential for controlling, monitoring, and enhancing various automotive operations. These systems typically consist of both hardware and software components, such as electronic control units (ECUs), sensors, actuators, and communication interfaces, which are responsible for tasks like engine management, safety features, infotainment, and advanced driver assistance systems \citep{navet2017, fairley2019}. 

\textbf{In-Vehicle Networks:} Modern vehicles may contain dozens or even hundreds of these embedded systems, interconnected through in-vehicle networks (e.g., CAN, LIN, FlexRay, Ethernet), enabling efficient communication and coordination among different vehicle subsystems \citep{bello2019, navet2017, fairley2019}. The design of in-vehicle embedded systems must address strict requirements for reliability, safety, real-time performance, and increasingly, cybersecurity, as these systems are critical to both vehicle operation and passenger safety \citep{bello2019, navet2017, mun2020}.

% Event-triggered logging and diagnostics - Done
\textbf{Event-Triggered Logging and Diagnostics:} Event-triggered logging and diagnostic frameworks, which record data only when anomalies or threshold crossings occur, are often adopted to reduce data transmission and avoid bus saturation in complex systems. However, this selective approach can reduce holistic visibility of system health, as it may miss subtle degradation patterns or early warning signs that do not cross predefined thresholds, complicating the detection of incipient faults and comprehensive condition monitoring \citep{nunes2023, jimenez2020, azar2022}. Additionally, the need to carefully tune event thresholds and diagnostic criteria introduces maintenance challenges, as improper settings can lead to missed events or excessive false positives, further complicating system upkeep and reliability \citep{nunes2023, azar2022}.

% Relevance of downstream ML tasks and data quantity problem - Done
\textbf{Downstream Machine Learning Tasks and Data Quantity:} Two developments in recent years further underline the shortcomings of event-triggered logging in automotive systems: the massive increase in signal-based data in the in-vehicle network and the growing relevance of downstream ML tasks. 

Recent industry and research reports indicate that the data quantity generated by ADAS (Advanced Driver Assistance Systems) sensors in vehicles is growing at an extremely rapid pace. According to a 2023 technical paper referencing McKinsey's 2021 automotive electronics report, by 2030, about 95 \% of new vehicles will be connected, up from around 50 \% today, and a single car can generate up to 1 terabyte (TB) of data per hour from its sensors \citep{bertoncello2021unlocking, samantaray2023}. This explosive growth is driven by the increasing number and sophistication of sensors—such as cameras, radars, and lidars—required for advanced safety and autonomous driving features, with the complexity and volume of data presenting significant challenges for storage, processing, and transmission within embedded automotive systems \citep{samantaray2023}.

Modern vehicles increasingly rely on data-driven intelligence to enhance safety, reliability and efficiency. Beyond perception and control, downstream ML tasks — those leveraging collected vehicle and sensor data for offline analysis, optimization and predictive functions — have become central to automotive-system design. These tasks include predictive maintenance \citep{theissler2021}, anomaly and intrusion detection \citep{oezdemir2024}, and fleet-level analytics like fuel consumption or maintenance scheduling \citep{app152011095}.

Recent reviews highlight that while event-triggered and anomaly-based data collection can optimize resource use, they often result in fragmented or incomplete datasets, making it harder to implement robust predictive maintenance strategies and limiting the effectiveness of ML models that rely on continuous, high-resolution data streams \citep{nunes2023, jimenez2020}. Multi-model and hybrid approaches are being explored to address these limitations, but the trade-off between data reduction and diagnostic completeness remains a significant challenge in both industrial and automotive contexts \citep{jimenez2020, azar2022}. % TODO: Maybe connect this to problem sections

% Limitations of traditional compression methods and introduction to compression - Done
\textbf{Traditional Compression:} Now, given the need for efficient data handling in the context of downstream ML tasks, and the shortcomings of event-triggered logging, one might look to traditional compression methods. 

Compression, as originated in information theory by \citet{shannon1948}, is the process of encoding information using fewer bits than the original representation. Compression techniques can be broadly categorized into lossless and lossy methods. Lossless compression is based on two principles: distribution modelling, sometimes called entropy modeling, and entropy coding. Entropy modeling involves creating a probabilistic representation of the data, while entropy coding assigns shorter codes to more frequent symbols based on their probabilities, thereby minimizing the average code length. Lossy compression, on the other hand, allows for some loss of information in exchange for higher compression ratios. This is typically achieved through techniques such as transform coding and quantization \citep{sayood2018}. For the purpose of this project, the focus will be on lossy compression as we focus on downstream ML tasks where some loss of fidelity is acceptable as long as the relevant information for the task is preserved.

Unfortunately, traditional compression methods, based on these information theory principles, often fall short in automotive applications, especially as a precursor for downstream ML tasks. For video/image compression traditional methods like JPEG or MP3 are optimized for human perception (e.g., visual quality) rather than ML tasks or efficient downstream data use \citep{ma2019}. For time series data, algorithmic approaches like CHIMP or Gorilla depend on manually chosen parameters like window size and are sensitive to data characteristics such as entropy and signal variability. This limits their effectiveness in capturing the nuances required for accurate ML model performance in automotive contexts. These algorithmic approaches where investigated by \citet{johnsson2025thesis} in a previous master thesis project upon which this work builds. % TODO: Clarify what we mean by building on

% Research directions - Done
\textbf{Related Research:} Existing research approaches these challenges from two different angles. First, utility-aware adaptive telemetry methods aim to employ policy learning methods to dynamically adjust telemetry parameters to reduce maintenance costs while preserving data utility for downstream tasks. While not an established practice yet, recent research has shown some promise \citep{zhang2023adapint}. Other research focuses on neural compression techniques that learn data representations optimized for both compression efficiency and ML task performance. This research is heavily inspired by deep generative models like GANs, VAEs, and autoregressive models, but focuses on compressing the data, instead of generating realistic data samples \citep{yang2022}. Here task-aware approaches have shown especially promising results as discussed in Section~\ref{sec:problem}. 

Neural compression techniques extend the introduced lossy compression principles in two key ways. First, they offer an alternative to traditional distribution modelling by leveraging deep neural networks to learn complex data distributions directly from the data, capturing intricate patterns and dependencies that traditional statistical models may miss. Second, they substitute traditional approaches to transform coding and quantization with learned representations \citep{yang2022}.
