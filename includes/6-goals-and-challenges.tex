\section{Goals and Challenges}\label{sec:goals-and-challenges}

\textbf{Goals:} In the proposed work, the goal is to determine whether using tokenization as an alternative to common transformation and quantization methods enables the subsequent entropy modeling to use a smaller architecture while still achieving similar results in compression rate and utility to those of traditional neural compression implementations. Instead of compressing raw sensor values, this approach would aim to learn a discrete vocabulary of prototypical temporal patterns that are maximally informative for downstream tasks. This approach is expected to give us two distinct advantages: 

\begin{itemize}
    \item better computational efficiency compared to RNN- and transformer-based neural compression methods.
    \item an interpretable intermediate layer of tokens instead of continuous values.
\end{itemize}

To achieve the proposed goal, the main focus of the project will be to develop a compression framework for time-series data using a learned tokenizer and lightweight entropy model. This project is expected to present the difference in predictive utility between models trained on data compressed using neural compression with tokenized inputs, data compressed using traditional neural compression techniques and uncompressed data. In addition, the project also aims to present the difference in computational cost and number of parameters for each approach. The expectation is that the approach which utilizes a small tokenizing module will have a smaller memory footprint and lower latency while still producing a compressed representation which offers comparable predictive utility. 

\textbf{Scope:} This project will focus on automotive time-series telemetry data. This includes multivariate as well as, in rare cases, multimodal time-series signal data. It does not cover image or video data. Furthermore, data from other domains will not be considered. For the downstream ML tasks, only a subset of prominent tasks will be considered. The aim is to generalize the implemented method over one well-established task type, such as predictive maintenance or anomaly detection. The specific task type will be chosen at a later date. Lastly, only lossy compression is within scope. There will be no evaluation of lossless compression methods. 

\textbf{Challenges:} Three main challenges are expected to be faced in this project. First, benchmarking is not established for neural compression methods for time-series data. This project relies on an established neural compression method to benchmark the proposed new approach against, which does not yet exist for time-series data. Such out-of-the-box solutions exist for image data, e.g., CompressAI by \citet{bégaint2020}, but available neural compression frameworks for time-series data are usually very specialized and not easily reproducible, e.g.~\citep{zheng2023, liu2024deepdictdeeplearningbased}. Second, we aim to implement a compression framework that generalizes beyond a specific downstream ML task (e.g., across anomaly detection tasks). This requires a common characterization of such tasks. Third, the focus on a broad field of downstream ML tasks also requires the appropriate fine-tuning of the tokenizer, which is expected to be a challenge.

\textbf{Approach:} To approach the goals for this project, data comprised of available automotive sensor and telemetry test-fleet data supporting tasks such as predictive maintenance and anomaly detection will be used. Alternatively, publicly available datasets such as the SCANIA Component X Dataset can be used \citep{kharazian2025}. We define the following tasks in order to achieve the aforementioned goals:

\begin{itemize}
    \item \textbf{Task 1:} Train a downstream ML model, representative of a defined subset of industry-relevant tasks, on uncompressed data to quantify loss in predictive utility.
    \item \textbf{Task 2:} Implement a baseline model based on established neural compression frameworks such as CompressAI \citep{bégaint2020}.
    \item \textbf{Task 3:} Develop a learnable tokenization module that discretizes data into semantically meaningful units optimized for downstream tasks.
    \item \textbf{Task 4:} Develop lightweight entropy modeling and coding schemes tailored to the tokenized representations.
    \item \textbf{Task 5:} Train the model from Task 1 on the compressed data using both compression methods. 
    \item \textbf{Task 6:} Evaluate baseline model and proposed tokenization + lightweight entropy model framework based on rate-utility and computational efficiency.
\end{itemize}