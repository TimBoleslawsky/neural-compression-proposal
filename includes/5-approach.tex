\section{Approach}\label{sec:approach}

\begin{itemize}
    \item \textbf{Dataset:} Use available automotive sensor and telemetry test-fleet data supporting tasks such as predictive maintenance and anomaly detection.
    \item \textbf{Task 1:} Train downstream ML models (NeuralCompression or CompressAI) on uncompressed data to quantify loss in predictive utility.
    \item \textbf{Task 2:} Implement established neural compression methods (TBC) as baselines, measuring rate-utility trade-offs.
    \item \textbf{Task 3:} Develop a learnable tokenization module that discretizes data into semantically meaningful units optimized for downstream tasks.
    \begin{itemize}
        \item Design tokenization schemes for automotive sensor data (time series).
        \item Define ML-aware utility metrics that correlate compression rate with downstream model performance (e.g., accuracy, F1-score).
    \end{itemize}
    \item \textbf{Task 4:} Evaluate and compare the methods.
    \begin{itemize}
        \item Measure rate-utility curves across the methods.
        \item Evaluate trade-offs between computational efficiency.
    \end{itemize}
    \item \textbf{Optional Task 5:} Evaluate the use of the tokenization framework as a precursor to neural compression methods, to further improve rate-utility trade-off. 
    \item \textbf{Expected Outcome:} Demonstrate that task-aware tokenization achieves comparable rate-utility trade-off to established neural compression approaches, while increasing computational efficiency. 
\end{itemize}
\citep{key}
